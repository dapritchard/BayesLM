
\documentclass[fleqn]{article}

\usepackage[margin=2cm]{geometry}
\usepackage{amsmath}
\usepackage{bm}                   % \bm (a bold font for math)
\usepackage{lmodern}              % remove font size warnings
\usepackage{enumitem}
\usepackage{titlesec}             % \titlespacing
\usepackage[hidelinks]{hyperref}  % appropriate page numbers for viewer
\usepackage{booktabs}



\renewcommand{\vec}{\bm}
\newcommand{\normal}{\mathcal{N}}
\newcommand{\ssr}{\text{SSR}}


% Typsetting package names
% \newcommand{\pkg}[1]{{\normalfont\fontseries{b}\selectfont #1}}
\newcommand{\pkg}[1]{\textsf{#1}}

%\newcommand{\lmr}{\textbf{\texttt{bayes\_lm\_r}}}
\def\lmr/{\textbf{\texttt{bayes\_lm\_r}}}
% \newcommand{\lmarma}{\textbf{\texttt{bayes\_lm\_arma}}}
\def\lmarma/{\textbf{\texttt{bayes\_lm\_arma}}}
% \newcommand{\lmeigen}{\textbf{\texttt{bayes\_lm\_eigen:}}}
\def\lmeigen/{\textbf{\texttt{bayes\_lm\_eigen}}}
% \newcommand{\lmrcpparma}{\textbf{\texttt{bayes\_lm\_rcpp\_arma}}}
\def\lmrcpparma/{\textbf{\texttt{bayes\_lm\_rcpp\_arma}}}
% \newcommand{\lmrcppeigen}{\textbf{\texttt{bayes\_lm\_rcpp\_eigen}}}
\def\lmrcppeigen/{\textbf{\texttt{bayes\_lm\_rcpp\_eigen}}}

% \def\arma/{\pkg{Armadillo}}
% \def\eigen/{\eigen/}
% \def\blas/{\pgk{BLAS}}
% \def\lapack/{\pkg{LAPACK}}
% \def\atlas/{\pkg{ATLAS}}
% \def\openblas/{\pkg{OpenBLAS}}

\def\arma/{\pkg{Armadillo}}
\def\eigen/{\pkg{Eigen}}
\def\blas/{\pkg{BLAS}}
\def\lapack/{\pkg{LAPACK}}
\def\atlas/{\pkg{ATLAS}}
\def\openblas/{\pkg{OpenBLAS}}

% Spacing before/after sections and subsections
\titlespacing*{\section}
{0pt}{5.5ex plus 1ex minus .2ex}{2.3ex plus .2ex}
\titlespacing*{\subsection}
{0pt}{7.5ex plus 1ex minus .2ex}{2.3ex plus .2ex}

% Typesetting "C++"
% \newcommand{\cpp}[1]{C\nolinebreak\hspace{-.05em}\raisebox{.4ex}{\tiny\bf +}\nolinebreak\hspace{-.10em}\raisebox{.4ex}{\tiny\bf +} #1}
% \def\cpp{{C\nolinebreak[4]\hspace{-.05em}\raisebox{.4ex}{\tiny\bf ++}}}
\def\cpp/{C\nolinebreak\hspace{-.05em}\raisebox{.4ex}{\tiny\bf +}\nolinebreak\hspace{-.10em}\raisebox{.4ex}{\tiny\bf +}}

% Typesetting "R"
\def\R/{\textsf{R}}

\newcommand{\sbar}{\,|\,}


\setenumerate{itemsep=1mm, topsep=2mm}
\setitemize{itemsep=1mm, topsep=2mm}




% Document start ---------------------------------------------------------------

\begin{document}

% Configure global options
<<setup, cache=FALSE, include=FALSE>>=
library(knitr)
knit_theme$set("moe")
opts_chunk$set(cache=TRUE)
system("rm ../CTime_Test_Arma.dat")
opts_knit$set(root.dir = normalizePath(".."))
pct <- function(x) { round(100 * x) }
@


\begin{titlepage}
  \centering
  {\scshape\LARGE A case study in the implementation of a Bayesian linear model
    using \textsf{R} and / or C++ libraries \par}
  \vspace{1cm}
  {\scshape\Large Including speed comparisons (and more to come) \par}
  \vspace{2cm}
  {\Large\itshape David Pritchard\par}

  \vfill
  {\large \today\par}
\end{titlepage}


\setcounter{section}{-1}
\section{Introduction}
As an exercise I decided to implement a Bayesian linear model using the \cpp/
libraries \arma/ and \eigen/, and then to interface the \cpp/ code
from \R/ via \pkg{Rcpp}.  The main purposes of this exercise were, as one might
guess from the choice of exercise, to obtain some familiarity with
\arma/, \eigen/, and \pkg{Rcpp}.

The reason for choosing the Bayesian linear model as the algorithm that I would
try to implement was that (i) at the time of writing this I have a Gibbs sampler
in hand that I wish to code in \cpp/ so the skills learned here will be
applicable to that problem, and (ii) this particular Gibbs sampler is a very
simple algorithm so that consequently the comparison of the implementations in
terms of speed is also relatively simple. \vspace{4mm}

\noindent There were also some secondary skills that I wanted to practice when
doing the exercise.  Some of these included:
\begin{itemize}
  \item Comparison of the speeds of the algorithm implementations
  \item Compare the speeds of using various implementations of \pkg{BLAS} and
    \pkg{LAPACK}
  \item Practice profiling programs in \R/ and in \cpp/
\end{itemize}




\subsection{Computer specifications}

The speed and profiling results presented in the document below will be highly
dependent on the computing environment on which they were obtained.  A short
summary of the specifications of the computing environment for which the results
in this document were obtained are listed below:

\begin{itemize}
\item Intel i3 64-bit processor
\item 2 hyper-threaded cores
\item 4 GB of main memory
\item 32 KB L1 cache
\item 256 KB L2 cache
\item 3 MB L3 cache
\item Ubuntu GNU/Linux 14.04 system
\end{itemize}

% Print the computer hardware specs
<<hardware-specs, engine="bash", size="footnotesize", include=FALSE>>=
sudo lshw -short
@

% Print the system specs
<<system-specs, engine="bash", size="footnotesize", include=FALSE>>=
cat /etc/*release
@




\section{The Bayesian linear model}
The Bayesian linear model and resulting algorithm which I consider follows the
definition and presentation of Peter Hoff's \emph{A First Course in Bayesian
  Statistical Methods}, sections 9.1 - 9.2.


\subsection{Model definition} \label{sec: model def}

Consider the model
\begin{equation} \vec{y} = \vec{X\beta} + \vec{\epsilon} \label{eq: linear model} \end{equation}
where
\[ \vec{\epsilon} \sbar \sigma^2 \sim \normal(\vec{0},\, \sigma^2 I) \]
\[ \sigma^2 \sim \text{inverse-gamma}(\nu_0 / 2,\, \nu_0 \sigma_0^2 / 2) \]
\[ \vec{\beta} \sim \mathcal{N}(\vec{\beta}_0,\, \vec{\Sigma}_0) \]


\subsection{Full conditional distributions} \label{sec: full conditional}

Then under the model presented in section \ref{sec: model def}, it can be shown
that the full conditional distributions are given by
\[ \sigma^2 \sbar \vec{y}, \vec{X}, \vec{\beta} \sim \text{inverse-gamma}\Big( [\nu_0 + n] / 2,~ \left[\nu_0 \sigma_0^2 + \ssr(\vec{\beta} \right] / 2 \Big) \]
\[ \vec{\beta} \sbar \vec{y}, \vec{X}, \vec{\beta} \sim \normal(\vec{m},\, \vec{V}) \]
where
\[ \ssr(\vec{\beta}) = (\vec{y} - \vec{X\beta})^{T} (\vec{y} - \vec{X\beta}) \]
\[ \vec{V} = \Big( \vec{\Sigma}_0^{-1} + \vec{X}^{T} \vec{X} / \sigma^2 \Big)^{-1} \]
\[ \vec{m} = \vec{V} \big( \sigma_0^{-1} \vec{\beta}_0 + \vec{X}^{T} \vec{y} / \sigma^2 \big) \]
Notice that the full conditional distribution for $\vec{\epsilon}$ is not mentioned - this is because it is completely determined conditional on $(\vec{y}, \vec{X}, \vec{\beta})$.  In fact, it is more natural in a Bayesian setting to rewrite (\ref{eq: linear model}) as $\vec{y} \sbar \vec{X\beta} \sim \normal (\vec{X\beta}, \sigma^2 I )$, which obviates any mention of $\epsilon$ from the model.



\subsection{Gibbs sampler algorithm} \label{sec: sampler algo}

It follows from section \ref{sec: full conditional} that an approximation for the joint posterior
distribution $p( \vec{\beta}, \sigma^2 \sbar \vec{y}, \vec{X} )$ can be obtained as follows:

\begin{enumerate}[leftmargin=*]
\item update $\vec{\beta}$:
  \begin{enumerate}
  \item compute $\vec{V}^{(s + 1)} = \Big( \vec{\Sigma}_0^{-1} + \vec{X}^{T} \vec{X} / \sigma^{2(s)} \Big)^{-1}$
  \item compute $\vec{m}^{(s + 1)} = \vec{V}^{(s+1)} \Big( \vec{\Sigma}_0^{-1} \vec{\beta}_0 + \vec{X}^{T} \vec{y} / \sigma^{2(s)} \Big)$
  \item sample $\vec{\beta}^{(s+1)} \sim \normal \big(\vec{m}^{(s+1)},\, \vec{V}^{(s+1)} \big)$
  \end{enumerate}
\item update $\sigma^2$:
  \begin{enumerate}
  \item compute SSR$(\vec{\beta}^{(s + 1)})$
  \item sample $\sigma^{2(s+1)} \sim \text{inverse-gamma}\Big( [\nu_0 + n] / 2,~ \left[\nu_0 \sigma_0^2 + \ssr(\vec{\beta}^{(s+1)}) \right] / 2 \Big)$
  \end{enumerate}
\end{enumerate}




\section{Progam descriptions}

% I wrote a total of five implementations of the Bayesian linear model described in section \ref{sec: model def}.  In brief, the implementations were as follows:
% \begin{itemize}
% \item An \R/-only version
% \item A \cpp/-only version using the \pkg{Armadillo} library
% \item A \cpp/-only version using the \eigen/ library
% \item An \R/ function that calls a \cpp/ function (\pkg{Armadillo} version) via \pkg{Rcpp}
% \item An \R/ function that calls a \cpp/ function (\eigen/ version) via \pkg{Rcpp}
% \end{itemize}

The source code for the following \R/ functions / executable programs is
contained in the root directory.  The name of each function / program is listed
as well as the files for each containing the source code.  Note that some of the
files are used for more than one implementation.

\begin{enumerate}[leftmargin=*]
\item \textbf{\texttt{bayes\_lm\_r:}} an \R/ function
  \begin{itemize}
  \item \texttt{Bayes\_LM.R}
  \item \texttt{Check\_Valid\_Input.R}
  \end{itemize}
\item \textbf{\texttt{bayes\_lm\_arma:}} a \cpp/-only implementation using the
  \arma/ library (an executable)
  \begin{itemize}
  \item \texttt{Bayes\_LM\_Arma.cpp}
  \item \texttt{Parse\_Args.cpp}
  \item \texttt{Stats\_Fcns\_Arma.cpp}
  \end{itemize}
\item \textbf{\texttt{bayes\_lm\_eigen:}} a \cpp/-only implementation using the
  \eigen/ library (an executable)
  \begin{itemize}
  \item \texttt{Bayes\_LM\_Eigen.R}
  \item \texttt{Parse\_Args.cpp}
  \item \texttt{Stats\_Fcns\_Eigen.cpp}
  \end{itemize}
\item \textbf{\texttt{bayes\_lm\_rcpp\_arma:}} an \R/ function internally calling
  a workhorse \cpp/ function constructed using the \arma/ library
  \begin{itemize}
  \item \texttt{Bayes\_LM\_Rcpp\_Arma.R}
  \item \texttt{Check\_Valid\_Input.R}
  \item \texttt{Stats\_Fcns\_Arma.cpp}
  \end{itemize}
\item \textbf{\texttt{bayes\_lm\_rcpp\_eigen:}} an \R/ function internally
  calling a workhorse \cpp/ function constructed using the \eigen/ library
  \begin{itemize}
  \item \texttt{Bayes\_LM\_Rcpp\_Eigen.R}
  \item \texttt{Check\_Valid\_Input.R}
  \item \texttt{Stats\_Fcns\_Eigen.cpp}
  \end{itemize}
\end{enumerate} \vspace{5mm}




\subsection{Writing \lmr/}

The function signature for \lmr/ is shown in the following.  The interfaces to
\lmrcpparma/ and \lmrcppeigen/ are similar.

<<lmr-function-signature, eval=FALSE>>=
bayes_lm_r <- function(n                = 100L,
                       p                = 15L,
                       nsamp            = 1e4,
                       prop_nonzero     = 0.2,
                       true_beta_sd     = 2,
                       true_sigma       = 2,
                       sd_x             = 2,
                       print_stats      = FALSE,
                       time_sections    = TRUE,
                       write_samples    = FALSE,
                       samples_file_loc = "Samples_R.dat",
                       decomp_method    = "chol") {}
@

\subsubsection{Brief description}

In short the function simulates the data $\vec{X}, \vec{y}$, and $\vec{\beta}$,
and then performs a Gibbs sampler for the model described in section \ref{sec:
  model def} using the simulated data.

\subsubsection{Fuller description}

In more detail, the function samples an $n \times p$ matrix $\vec{X}$ with each
element $X_{ij}$ independently sampled from a normal distribution with mean 0
and standard deviation as specified by \texttt{sd\_x}.  A $p$-length vector
$\vec{\beta}$ is sampled with \texttt{prop\_nonzero} proportion of its entries
having values sampled from a normal distribution with mean 0 and standard
deviation \texttt{true\_beta\_sd}; the remaining values are set to 0.  Then
$\vec{y}$ is sampled from a normal distribution with mean $\vec{X\beta}$ and
variance $\sigma^2 I$ where $\sigma$ is specified by \texttt{true\_sigma}.

A Gibbs sampler with \texttt{nsamp} scans is then performed.  If specified by
\texttt{print\_stats} then upon completion, a summary of the sample quantiles
will be written to the \R/ console, and if specified by \texttt{write\_samples}
then each sample will be written to a file with filename as specified by
\texttt{samples\_file\_loc}.

The Gibbs sampler algorithm (described in section \ref{sec: sampler algo})
requires sampling from a multivariate normal distribution; doing so requires
performing a matrix decomposition.  The function provides the option to use
either the Cholesky decomposition or the Eigen decomposition for this part of
the algorithm, the choice of which can be specified by \texttt{decomp\_method}.

The formal argument \texttt{time\_sections} allows the user the option to track
the total time spent in the sampler (i) performing matrix inversions (ii)
sampling from the multivariate normal distribution.  The algorithm requires one
execution of each task per scan.

\lmr/ returns a length-3 vector with values providing the time in the sampler
spent performing the inversion, the multivariate normal sampling, and the
overall time in the sampler.  Note that this overall time includes only the
sampler and not any computations performed before or after.

\subsubsection{Coding observations}

This is quite a simple model to code (the model was chosen for this reason!).
Outside of checking the function arguments for validity, the function spans
about 150 lines of code, including comments and blank lines!  \R/ provides all of
the tools we need for this particular program and makes life very nice and
simple for us.


\subsection{Writing \lmarma/}

To invoke run the Bayesian linear model sampler using a \cpp/-only version now
of course is done from the command-line.  To obtain the default parameter
specifications (i.e. the specifications obtained by invoking
<<arma-noargs, engine="bash", eval=FALSE>>=
./bayes_lm_arma
@
\noindent one could explicitely specify each parameter by
<<arma-defargs, engine="bash", eval=FALSE>>=
./bayes_lm_arma -n=100  \
                -p=15  \
                -prop_nonzero=0.2  \
                -true_beta_sd=2  \
                -sd_x=2  \
                -nsamp=1000  \
                -print_stats=false  \
                -write_ctime=false  \
                -ctime_file_loc=Comp_Time_Arma.dat  \
                -write_samples=false  \
                -samples_file_loc=Samples_Arma.dat  \
                -decomp_method=c  \
                -set_seed=22
@

\noindent Actually, if left unspecified the seed is generating using the CPU
clock so there isn't really a default - that way running the program without
specifiying a seed generates a different sequence every time.  The number 22 was
arbitrarily chosen here as an example.


\subsubsection{Overall impression}

This was my first exposure to the \arma/ library.  I found it to be
very intuitive and convenient to work with.  The documentation was relatively
clear and helpful.  If I could offer one suggestion for room for improvement
regarding the documentation, I wish that it would provide a little more detail
on the underlying algorithms that are being used to perform the provided
functionality.

\subsubsection{Choosing a random number generating method}

One decision that has to be made when constructing a sampler in \cpp/ is how to
generate the random numbers.  While there are several options, such as GNU's
random number generation (RNG) library, or the \cpp/ 11 library, I ended up using
\R/'s RNG library for the following reasons: (i) I am very familiar with the \R/
RNG interface, and (ii) we are not allowed to make use of the system RNG for \R/
packages submitted to CRAN (which disqualifies the GNU and \cpp/ 11 libraries).
The available options and merits of the available RNG libraries is a topic which
I will have to investigate in more detail in future, however.

\subsubsection{Converting from \R/ code to   \texttt{C++}}

Now that we're not in the friendly confines of the \R/ programming environment,
there was some additional work that needed to be performed to achieve the same
functionality.

\begin{enumerate}[leftmargin=*]
\item The \arma/ library provides a method to randomly populate a
  matrix with independent standard normal entries, however it does not provide a
  method to sample from an arbitrary multivariate normal distibution.
  Consequently I had to write a multivariate normal sampling function - I wrote
  one using the Cholesky decomposition, and one using the Eigen decompostion.
  Each of these functions required just a few lines of code.

\item I also had to construct a quantile function (\arma/ provides a
  median quantile method, but not for arbitrary quantiles).  I just emulated the
  default quantile function from \R/; this required about a dozen lines of code.

\item Another change that was required going from \R/ to \cpp/ was reading in
  the function arguments.  In \R/ all of the work that is performed parsing the
  function arguments is taken care of for me by \R/; in \cpp/ we have to do a
  little more manual work to read the function arguments (by arguments here I
  mean command-line arguments).  Of course there are some extremely powerful
  tools provided by the C standard, so thankfully we don't have to do much work
  from scratch.  However there are still some mundane work that need to be
  performed when parsing the arguments - this required about 150 lines of code.
  In comparison the \R/ code that I wrote for argument checking was about 75
  lines.

\item One thing that confused me about using the \R/ math library for random
  number generation was how to properly set up the random number stream and set
  the seed of the stream.  In the \emph{Writing R Extensions} manual section
  6.3, it says that all calls to RNG routines should be prefaced by a call to
  \texttt{GetRNGstate()}, and that after all the required variates have been
  generated, \texttt{GetRNGstate()} should be called.  However, when using the
  \R/ math library as a standalone library, the random number stream is
  automatically set up for you, and setting the seed is accomplished through
  \texttt{set\_seed} (declared in \texttt{Rmath.h}).  See the \emph{Writing R
    Extensions} manual section 6.16 for more details.
\end{enumerate}

\subsubsection{Compiling \texttt{C++} code with \arma/} \label{sec: compiling with arma}

The source code can be compiled as follows.
<<compiling-arma, engine="bash", eval=FALSE>>=
g++ Bayes_LM_Arma.cpp Parse_Args.cpp Stats_Fcns_Arma.cpp    \
    -DMATHLIB_STANDALONE -Wall -g3 -O3 -lRmath -larmadillo  \
    -o bayes_lm_arma
@

\noindent The first three arguments are filenames of the source code that needs
to be pulled into the compilation.  The argument \texttt{-DMATHLIB\_STANDALONE}
defines the MATHLIB\_STANDALONE macro which in turn configures the
\texttt{Rmath.h} header for use with the Rmath standalone library.
\texttt{-Wall} tells \texttt{gcc} to provide additional warnings for potentially
problematic constructs, \texttt{-g3} includes debugging information in the
resulting binary file, and \texttt{-O3} requests optimization from the compiler
(note that this optimization flag is very important for heavily templated code
such as the \arma/ and \eigen/ libraries).  The arguments
\pkg{-lRmath} and \pkg{-larmadillo} tell the compiler the link the files against
the \R/ math library and \arma/ library, respectively.  The argument
\texttt{-o bayes\_lm\_arma} specifies the filename of the resulting executable.



\subsection{Writing \lmeigen/}

\subsubsection{Overall impression}

This was the second time that I had any exposure to \eigen/.  I find it also
to be quite intuitive and convenient to work with.  I thought the documentation
was well-developed and quite thorough.  If I had to mention one area that I
found a little not to my taste, it was that many of the classes and methods had
very verbose names (e.g. \texttt{transpose()} vs. \texttt{t()}).

\subsubsection{Converting \arma/ code to \eigen/}

Converting \arma/ to \eigen/ was quite straightforward; in many
places I just had to change e.g. \texttt{arma::mat} to \texttt{Eigen::MatrixXd}.
There were a few differences, however.

\begin{enumerate}[leftmargin=*]
\item \eigen/ does not have a method to randomly populate a matrix with
  independent standard normal entries, so I had to create one; this requires
  just a handful of lines of code.

\item The easy way to calculate the empirical quantiles of the samples is to
  sort the data beforehand.  As opposed to \arma/, there was not a
  method for sorting the rows or columns of a matrix, so this requires just
  bringing in e.g. \texttt{qsort} or \texttt{std::sort}.

  As a aside, in order to calculate the emirical quantiles of interest, it is
  not required to sort the entirety of the data, and there are fast algorithms
  for obtaining the $k$-th largest item in a set.  However in the interests of
  (the programmer's) time and simplicity, I just performed a total sort.

\item There wasn't a built-in matrix inverse method that leveraged the fact that
  the matrix that we are inverting is a symmetric postive-definite matrix.
  Instead, this required me to use the \texttt{solve} method for the Cholesky
  decomposition, where we are solving for the identity matrix.  It would seem
  that a dedicated method for the inverse could potentially be more efficient.
  The speed comparisons in section \ref{sec: p increase} seem to support this
  possibility.
\end{enumerate}


\subsubsection{Compiling \texttt{C++} code with \eigen/}

The source code can be compiled as follows.
<<compiling-eigen, engine="bash", eval=FALSE>>=
g++ Bayes_LM_Eigen.cpp Parse_Args.cpp Stats_Fcns_Eigen.cpp  \
    -DMATHLIB_STANDALONE -Wall -g3 -O3 -lRmath              \
    -o bayes_lm_eigen
@

\noindent All of the options here have been previously discussed in section
\ref{sec: compiling with arma}.  It is worth noting however, that for
\eigen/ (as apposed to \arma/) we do not link it against a shared
library, instead it is statically linked as part of the compilation process.


\subsection{Writing \lmrcpparma/ and \lmrcppeigen/}

The steps to convert the \cpp/ code using the \arma/ library to a function for
use through \R/ were exactly the same as for converting \cpp/ code using the
\eigen/ library, so I have included them in the same section.  Any mention of
\arma/ can be replaced by \eigen/.

\subsubsection{Overall impression}

This was my first time using \pkg{Rcpp}.  It is incredible how convenient it makes it to
interface \cpp/ code from \R/.

\subsubsection{Interfacing \arma/ code through \texttt{R}} \label{sec: rcpp arma}

Essentially all of the pieces were already written at this point, and it was
just a matter of putting them all together.  Now that we are back in \R/-land,
I'd prefer and am able to do the argument checking in \R/ again.  So the
user-facing function simply checks the function arguments for validity and does
any necessary preprocessing, and then calls an \R/ function pointing to a
function in the dynamically loaded library constructed from the \cpp/ code,
supplying it with the now sanitized data.  The \cpp/ function pointed to by the
caller is a thin wrapper constructed by \pkg{Rcpp}, which takes all of the
\texttt{SEXP} pointers passed to it from \R/ and creates \texttt{Rcpp:RObject}s
from them, and in turn calls our \cpp/ function, providing these
\texttt{Rcpp:RObject} objects as the arguments. \vspace{4mm}

\noindent To put all of the pieces together then mainly required:
\begin{enumerate}[leftmargin=*]

\item Creating the \cpp/ function
  \begin{enumerate}
  \item Creating a function signature for the \cpp/ function using \cpp/
    primatives and \texttt{Rcpp::RObject} derivatives
  \item Copy-pasting the body of the original \arma/ code into the
    function
  \item Creating a return argument for the function, again using \cpp/ primatives
    and \texttt{Rcpp::RObject} derivatives
  \end{enumerate}

\item Creating the \R/ wrapper
  \begin{enumerate}
  \item Copy-pasting the function signature and argument-checking lines of the
    original \R/ function
  \item Compiling the the \cpp/ code, loading the newly created shared library
    into memory, and obtaining a pointer to it, all through a call to
    \texttt{Rcpp::sourceCpp}
  \end{enumerate}

\item Modifying the file containing the statistical helper functions so that it
  would work with both code being called from \R/ and as part of a standalone
  \cpp/ program
  \begin{enumerate}
  \item The issue that I had was that that the \texttt{rnorm} function
  wasn't declared when compiling the functions with the \texttt{Rcpparmadillo.h}
  header (as opposed to the \texttt{Rmath.h} header).  In the
  \texttt{RcppArmadillo.h} header the authors map the alias \texttt{R::rnorm} to the
  \texttt{rnorm} function, so what I did as a solution was to create an alias of
  this same name for the case when the file is included as part of a standalone
  \cpp/ program, and that way I can call the desired function by
  \texttt{R::rnorm} in both cases.
  \end{enumerate}
\end{enumerate}




\section{Comparison of program speeds}

The speed in which Bayes linear model sampler could be completed was measured
using 5 different software configurations:

\begin{center} \setlength{\tabcolsep}{20pt}
  \begin{tabular}{@{}ll@{}}
    \toprule
    Configuration & Details \\
    \midrule
    \R/ + \atlas/        & \lmr/ using \atlas/ version of both \blas/ and \lapack/ \\[1ex]
    \R/ + \openblas/     & \lmr/ using \openblas/ version of \blas/ and reference \lapack/ \\[1ex]
    \arma/ + \atlas/    & \lmrcpparma/ using \atlas/ version of both \blas/ and \lapack/ \\[1ex]
    \arma/ + \openblas/ & \lmrcpparma/ using \openblas/ version of \blas/ and reference \lapack/ \\[1ex]
    \eigen/             & \lmrcppeigen/ (does not use \blas/ / \lapack/) \\
    \bottomrule
  \end{tabular}
\end{center}



% \noindent Some explanation of what is meant here is needed.  \R/ + \atlas/ means that
% the runtime of \lmr/ was measured using the \atlas/ version of both
% \blas/ and \lapack/.  Similarly, \R/ + \openblas/ means that the
% runtime of \lmr/ was measured using the \openblas/ version of \blas/ and
% reference \lapack/.  \arma/ + \atlas/ and \arma/ +
% \openblas/ means that the runtime of \lmrcpparma/ was measured using
% \atlas/ / \openblas/ libraries.  Finally, \eigen/ means that the
% runtime of \lmrcppeigen/ was measured (note that \eigen/ does not use
% \blas/ / \lapack/ routines).

\noindent Caveat: to obtain peak efficiencies of \atlas/ and \openblas/ the
libraries should be compiled on the local machine, however these tests were run
using the \atlas/ and \openblas/ Debian packages.


\newpage
\subsection{Runtimes as $n$ increases} \label{sec: n increase}

\includegraphics[width=1\textwidth]{figure/n_increase} \vspace{2mm}

\noindent Observations / impressions regarding the speed comparisons:

\begin{itemize}[leftmargin=*]

\item The matrix operations are the dominating cost in performing the Gibbs
  sampler as $n$ gets comparatively large.  The main cost here is incurred by
  calculating $|| \vec{y} - \vec{X\beta}^{(s)} ||^2$.

\item The cost in calculating the matrix inverse and sampling from the
  multivariate normal distribution mostly is a function of $p$, so we see that
  the time required for these sections remains mostly constant.

\item The \R/-``only'' functions are essentially dominated by the code calling
  \cpp/-workehorse functions, by a factor of about 5-6 times for the overall
  running time.

\item The parallel processing done for performing the matrix inverse and
  sampling the multivariate normal distribution is slower for the \arma/ +
  \openblas/ than for the sequential processing done for \arma/ + \atlas/ or
  \eigen/ by about a factor of 1.5 or 2.5 times, respectively.  Since these
  operations are dependent mostly on the size of $p$ which is relatively small
  here, it seems that the overhead required in the construction and reductions
  required for parallel processing outweighs the gain incurred by executing
  instructions concurrently.

\item In a comparison of the functions calling sequential \cpp/ code, \arma/ +
  \atlas/ is dominated by \eigen/.

\item \arma/ + \openblas/ does comparatively poorly for small sizes of $n$,
  again presumably due to the overhead incurred for parallel processing.  When
  $n$ gets large \arma/ + \openblas/ does better than \arma/ + \atlas/, by a
  factor of about 3.5.

\item \eigen/ is at least tied for the fastest for every size of $n$ observed.

\end{itemize}


\newpage
\subsection{Runtimes as $p$ increases} \label{sec: p increase}

\includegraphics[width=1\textwidth]{figure/p_increase}\vspace{6mm}

\noindent Observations / impressions regarding the speed comparisons:

\begin{itemize}[leftmargin=*]

\item Calculating the matrix inverse and sampling from the multivariate normal
  distribution are the dominating costs in performing the Gibbs sampler as $p$
  gets comparatively large.

\item The \R/-``only'' functions are dominated by the code calling
  \cpp/-workehorse functions that use the \arma/ library, by a factor of about 4
  times for the overall running time.  However, \R/ + \openblas/ outperforms
  \eigen/ when calculating the matrix inverse for large $p$ by about a factor of
  2.

\item \arma/ + \openblas/ does comparatively poorly for small sizes of $p$,
  again presumably due to the overhead incurred for parallel processing (with
  the interesting exception of the matrix operations section).  However when $p$
  gets large \arma/ + \openblas/ has the best performance, and does better than
  the second-best competitor, \arma/ + \atlas/, by a factor of about 2.

\item The bottleneck for \eigen/ seems to be the solve method for the Cholesky
  decomposition.  This statement is based on the fact that the cost of the
  multivariate normal sampling section is about equal for \eigen/ compared to
  \arma/ + \atlas/, and which consists of a Cholesky decomposition and matrix
  multiplication.  However, the matrix inverse section for \eigen/, which
  consists of a Cholesky decomposition and a solve method, performs worse than
  the \arma/ + \atlas/ program.

  Perhaps this is due to the fact that this is a general solve method for a
  Cholesky decomposition - i.e. we solve $\vec{LL}^{T}\vec{X} = \vec{B}$ for
  $\vec{X}$, where we choose $\vec{B}$ to be the identity matrix and $\vec{L}$
  is the Cholesky decomposition of the matrix of interest.  However it would
  seem that a method that is specialized for calculating the inverse would be
  far more efficient.  Is there a better way to do it in \eigen/?

\end{itemize}








\end{document}
